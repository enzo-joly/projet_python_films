{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84943756",
   "metadata": {},
   "source": [
    "# Systèmes de Recommandation avec la base de données MovieLens 1M \n",
    "\n",
    "## 1). Importer en nettoyer le jeu de données MovieLens 1M\n",
    "\n",
    "Dans cette section, nous utilisons le jeu de données **MovieLens 1M**, une base de données publique contenant **1 million de notations** de films réalisées par environ **6 000 utilisateurs** sur plus de **3 900 films**.\n",
    "\n",
    "Ces données sont largement utilisées pour l’expérimentation et la recherche dans les **systèmes de recommandation** basés sur les filtrages **collaboratif** et **content-based** que nous allons implémenter dans cette section du projet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfbff521",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "from helper_functions import download_movielens, create_df, search_films\n",
    "\n",
    "# Set Global Chart Style\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "palette = sns.color_palette(\"viridis\")\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['axes.titlesize'] = 14\n",
    "plt.rcParams['axes.labelsize'] = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421ea953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading the data (this can take up to 5 minutes to run)\n",
    "if __name__ == \"__main__\":\n",
    "    download_movielens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00d024c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging users info + movie info + ratings, cleaning up the df \n",
    "df_users = create_df()\n",
    "df_users.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4448c1e7",
   "metadata": {},
   "source": [
    "## 2). Visualiser les données (Exploratory Data Analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523cd3ae",
   "metadata": {},
   "source": [
    "### Démographie et utilisateurs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee006bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nombre total d'utilisateurs\n",
    "total_users = df_users[\"UserID\"].nunique()\n",
    "print(\"Total number of unique users:\", total_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ea2eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution des utilisateurs par Sexe\n",
    "gender_counts = df_users.groupby(\"Gender\")[\"UserID\"].nunique().rename({1: \"Female\", 0: \"Male\"})\n",
    "print(gender_counts)\n",
    "\n",
    "print(\" \")\n",
    "print(f\"There are {gender_counts[\"Male\"]/(gender_counts['Female']+gender_counts[\"Male\"])*100}% men in the dataset\")\n",
    "\n",
    "plt.figure(figsize=(4, 3))\n",
    "gender_counts.plot(kind=\"bar\", color=[\"steelblue\", \"lightcoral\"])\n",
    "plt.title(\"Gender Distribution\")\n",
    "plt.xlabel(\"Gender\")\n",
    "plt.ylabel(\"Number of Users\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9342eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution par Tranche d'Âge\n",
    "age_map = {1: \"Under 18\", 18: \"18-24\", 25: \"25-34\", 35: \"35-44\", 45: \"45-49\", 50: \"50-55\", 56: \"56+\"}\n",
    "df_users['AgeGroup'] = df_users['Age'].map(age_map)\n",
    "age_order = [\"Under 18\", \"18-24\", \"25-34\", \"35-44\", \"45-49\", \"50-55\", \"56+\"]\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.countplot(data=df_users.drop_duplicates('UserID'), x='AgeGroup', order=age_order, palette=\"viridis\")\n",
    "plt.title(\"Distribution par Tranche d'Âge\")\n",
    "plt.xlabel(\"Groupe d'âge\")\n",
    "plt.ylabel(\"Nombre d'utilisateurs\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8a9c6d",
   "metadata": {},
   "source": [
    "**Quelques conclusions :** \n",
    "* Déséquilibre Homme/Femme : Le jeu de données présente une majorité d'utilisateurs masculins (environ 71.7%).\n",
    "\n",
    "* Prédominance des jeunes adultes : La tranche d'âge la plus représentée est celle des 25-34 ans, suivie par les 18-24 ans. Cela indique que la base de données reflète principalement les goûts d'une population jeune et active au moment de la collecte."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70dcb3f5",
   "metadata": {},
   "source": [
    "### Les films "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58173833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution des films par année\n",
    "df_unique_films = df_users.drop_duplicates('MovieID')\n",
    "\n",
    "# Comptage des films par année\n",
    "films_par_an = df_unique_films['Year'].value_counts().sort_index()\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "sns.lineplot(x=films_par_an.index, y=films_par_an.values, color='teal', linewidth=2.5)\n",
    "plt.fill_between(films_par_an.index, films_par_an.values, color='teal', alpha=0.15)\n",
    "\n",
    "plt.title(\"Évolution du nombre de films par année de sortie\", fontsize=16)\n",
    "plt.xlabel(\"Année de sortie\", fontsize=12)\n",
    "plt.ylabel(\"Nombre de films uniques\", fontsize=12)\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbb4e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution des films par genre \n",
    "df_unique_movies = df_users.drop_duplicates(subset=['MovieID'])\n",
    "df_exploded = df_unique_movies.explode(\"Genres\")\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.countplot(x='Genres', data=df_exploded, hue='Genres', palette=\"crest\")\n",
    "plt.title(\"Nombre Total de Films par Genre\", fontsize=16)\n",
    "plt.xlabel(\"Genres de films\", fontsize=12)\n",
    "plt.ylabel(\"Nombre de films\", fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81312860",
   "metadata": {},
   "source": [
    "### Les notes des utilisateurs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba73c386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10 des Films les plus notés\n",
    "plt.figure(figsize=(6,4))\n",
    "top_movies = df_users['Title'].value_counts().head(10)\n",
    "sns.barplot(x=top_movies.values, y=top_movies.index, hue=top_movies.index, palette=\"magma\", legend=False)\n",
    "plt.title(\"Top 10 des Films les plus populaires (Nombre de notes)\")\n",
    "plt.ylabel(\"Titre\")\n",
    "plt.xlabel(\"Nombre de notations\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3909a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution des Notes attribuées\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.countplot(x='Rating', data=df_users, palette=\"crest\", hue='Rating', legend=False)\n",
    "plt.title(\"Fréquence des notes (échelle de 1 à 5)\")\n",
    "plt.xlabel(\"Note\")\n",
    "plt.ylabel(\"Nombre de notes\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3144709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notes moyennes par sexe \n",
    "\n",
    "# 0 = Male, 1 = Female \n",
    "moyennes_globales = df_users.groupby('Gender')['Rating'].mean().reset_index()\n",
    "moyennes_globales['Sexe'] = moyennes_globales['Gender'].map({0: 'Homme', 1: 'Femme'})\n",
    "\n",
    "plt.figure(figsize=(4, 3))\n",
    "ax = sns.barplot(data=moyennes_globales, x='Sexe', y='Rating', palette=['#5dade2', '#ec7063'], hue='Sexe', legend=False)\n",
    "\n",
    "for p in ax.patches:\n",
    "    ax.annotate(format(p.get_height(), '.2f'), \n",
    "                (p.get_x() + p.get_width() / 2., p.get_height()), \n",
    "                ha = 'center', va = 'center', \n",
    "                xytext = (0, 9), \n",
    "                textcoords = 'offset points',\n",
    "                fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.title(\"Note Moyenne Globale : Hommes vs Femmes\", fontsize=15)\n",
    "plt.ylabel(\"Note Moyenne (1-5)\", fontsize=12)\n",
    "plt.xlabel(\"Sexe\", fontsize=12)\n",
    "plt.ylim(0, 5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0703a1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Préférences : Notes moyennes par genre et pas sexe \n",
    "# Explode the genres \n",
    "df_exploded = df_users.explode(\"Genres\")\n",
    "\n",
    "# Calculating the average rating per genre \n",
    "stats_moyennes = df_exploded.groupby([\"Genres\", \"Gender\"])[\"Rating\"].mean().reset_index()\n",
    "\n",
    "stats_moyennes[\"Sexe\"] = stats_moyennes[\"Gender\"].map({0: \"Homme\", 1: \"Femme\"})\n",
    "\n",
    "# Rank the genres by total average rating\n",
    "ordre_genres = df_exploded.groupby(\"Genres\")[\"Rating\"].mean().sort_values(ascending=False).index\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 4))\n",
    "ax = sns.barplot(\n",
    "    data=stats_moyennes,\n",
    "    x=\"Genres\",\n",
    "    y=\"Rating\",\n",
    "    hue=\"Sexe\",\n",
    "    palette=[\"#5dade2\", \"#ec7063\"], \n",
    "    order=ordre_genres\n",
    ")\n",
    "\n",
    "plt.title(\"Note Moyenne par Genre et par Sexe (MovieLens 1M)\", fontsize=16)\n",
    "plt.xlabel(\"Genres de films\", fontsize=12)\n",
    "plt.ylabel(\"Note Moyenne (1-5)\", fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.ylim(0, 5) \n",
    "plt.legend(title=\"Sexe\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55cfb126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse des Volumes : Comptage des notations par genre et par sexe\n",
    "df_exploded = df_users.explode(\"Genres\")\n",
    "\n",
    "stats_comptage = df_exploded.groupby([\"Genres\", \"Gender\"]).size().reset_index(name='Nombre de Notations')\n",
    "\n",
    "stats_comptage[\"Sexe\"] = stats_comptage[\"Gender\"].map({0: \"Homme\", 1: \"Femme\"})\n",
    "\n",
    "ordre_popularite = df_exploded['Genres'].value_counts().index\n",
    "\n",
    "# Visualisation\n",
    "plt.figure(figsize=(10, 4))\n",
    "sns.barplot(\n",
    "    data=stats_comptage,\n",
    "    x=\"Genres\",\n",
    "    y=\"Nombre de Notations\",\n",
    "    hue=\"Sexe\",\n",
    "    palette=[\"#5dade2\", \"#ec7063\"], \n",
    "    order=ordre_popularite\n",
    ")\n",
    "\n",
    "plt.title(\"Nombre Total de Notations par Genre et par Sexe\", fontsize=16)\n",
    "plt.xlabel(\"Genres de films\", fontsize=12)\n",
    "plt.ylabel(\"Nombre de notations (Volume)\", fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "plt.legend(title=\"Sexe\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b7de94",
   "metadata": {},
   "source": [
    "**Quelques conclusions :**\n",
    "\n",
    "**Comportement de Notation :** La note 4 est la plus fréquente. La popularité est concentrée sur quelques blockbusters.\n",
    "\n",
    "**Préférences par Genre :**\n",
    "* Les genres Film-Noir, War et Documentary obtiennent les meilleures notes moyennes globales (souvent supérieures à 4/5) pour les deux sexes.\n",
    "* Les hommes mettent des notes moyennes plus basses sur des genres comme Romance ou Musical. \n",
    "\n",
    "Par la suite, nous allons nous concentrer surtout sur les genres. Les données démographiques (sexe, âge, code postal) servent à vérifier la représentativité de l'échantillon mais ne sont pas incluses dans les modèle de filtrage collaboratif et content-based. Ces modèles reposent exclusivement sur les interactions passées pour prédire les goûts futurs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50752d99",
   "metadata": {},
   "source": [
    "## 3). Modelling : Approche par Filtrage Collaboratif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb1de88",
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_id, movie_title = search_films(\"when harry met sally\", df_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1901f58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f2f5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b5d264",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d570239b",
   "metadata": {},
   "source": [
    "### 3.1). Préparation des matrices de données Y et R\n",
    "\n",
    "Pour implémenter l'algorithme de filtrage collaboratif, nous devons d'abord transformer nos données sous forme matricielle :\n",
    "\n",
    "- $Y$ est une matrice de dimensions $(n_m \\times n_u)$ contenant les notes, où $n_m$ est le nombre de films et $n_u$ le nombre d'utilisateurs.\n",
    "\n",
    "- $R$ est une matrice binaire de même dimension, où $R(i,j)=1$ si l'utilisateur $j$ a noté le film $i$, et $0$ sinon.\n",
    "\n",
    "La prédiction d'une note est définie par le produit scalaire des vecteurs de caractéristiques :\n",
    "\n",
    "$$\n",
    "\\text{Note Prédite}(i,j) = x^{(i)} \\cdot w^{(j)} + b^{(j)}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72797054",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création de la matrice de notes Y (Films en lignes, Utilisateurs en colonnes)\n",
    "Y_df = df_users.pivot_table(index='MovieID', columns='UserID', values='Rating').fillna(0)\n",
    "Y_matrix = Y_df.values\n",
    "R_matrix = (Y_matrix > 0).astype(int)\n",
    "\n",
    "# Conversion en tenseurs PyTorch pour le calcul manuel\n",
    "Y = torch.tensor(Y_matrix, dtype=torch.float32)\n",
    "R = torch.tensor(R_matrix, dtype=torch.float32)\n",
    "\n",
    "num_movies, num_users = Y.shape\n",
    "num_features = 200  # Nombre de caractéristiques latentes (nf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c61fdb",
   "metadata": {},
   "source": [
    "### 3.2). Initialisation manuelle des paramètres \n",
    "\n",
    "Nous initialisons directement les matrices de paramètres que nous allons apprendre par descente de gradient :\n",
    "\n",
    "- $X$ : matrice des caractéristiques des films de dimension $(n_m \\times n_f)$.\n",
    "\n",
    "- $W$ : matrice des préférences des utilisateurs de dimension $(n_u \\times n_f)$.\n",
    "\n",
    "- $b$ : vecteur de biais des utilisateurs de dimension $(1 \\times n_u)$.\n",
    "\n",
    "Nous activons `requires_grad=True` pour permettre à PyTorch de suivre ces tenseurs et de calculer automatiquement les gradients de la fonction de coût par rapport à chacun d'eux.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf11f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisation aléatoire des tenseurs\n",
    "X = torch.randn(num_movies, num_features, requires_grad=True)\n",
    "W = torch.randn(num_users, num_features, requires_grad=True)\n",
    "b = torch.randn(1, num_users, requires_grad=True)\n",
    "\n",
    "# Optimiseur Adam pour mettre à jour nos matrices X, W et b\n",
    "optimizer = torch.optim.Adam([X, W, b], lr=0.1)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=40, gamma=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e2517f",
   "metadata": {},
   "source": [
    "**Résumé des matrices créées :**\n",
    "\n",
    "\n",
    "| Paramètre | Symbole | Taille | Description |\n",
    "| :--- | :--- | :--- | :--- |\n",
    "| **Caractéristiques Films** | $\\mathbf{X}$ | $(\\text{nm} \\times \\text{nf})$ | $nm$: Nombre de films (3706). $nf$: Nombre de caractéristiques (200). |\n",
    "| **Paramètres Utilisateurs** | $\\mathbf{W}$ | $(\\text{nu} \\times \\text{nf})$ | $nu$: Nombre d'utilisateurs (6040). $nf$: Nombre de caractéristiques (200). |\n",
    "| **Biais Utilisateurs** | $\\mathbf{b}$ | $(1 \\times \\text{nu})$ | Ajustement de la note pour chaque utilisateur. |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065e806e",
   "metadata": {},
   "source": [
    "### 3.3). Implémentation de la fonction de coût $J$\n",
    "\n",
    "La fonction de coût mesure l'erreur entre les notes réelles et nos prédictions, avec l'ajout d'une régularisation $L_2$ pour éviter le surapprentissage :\n",
    "\n",
    "$$\n",
    "J(X, W, b) =\n",
    "\\frac{1}{2}\n",
    "\\sum_{(i,j):\\,R(i,j)=1}\n",
    "\\left( x^{(i)} \\cdot w^{(j)} + b^{(j)} - y(i,j) \\right)^2\n",
    "+\n",
    "\\frac{\\lambda}{2}\n",
    "\\sum_{j} \\|w^{(j)}\\|^2\n",
    "+\n",
    "\\frac{\\lambda}{2}\n",
    "\\sum_{i} \\|x^{(i)}\\|^2\n",
    "$$\n",
    "\n",
    "Cette fonction est implémentée de manière vectorisée pour optimiser les performances de calcul.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7804053c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cofi_cost_func(X, W, b, Y, R, lambda_):\n",
    "    # Calcul de toutes les prédictions : (X @ W.t()) + b\n",
    "    # Shape résultante : (num_movies, num_users)\n",
    "    predictions = (X @ W.t()) + b\n",
    "    \n",
    "    # Calcul de l'erreur uniquement là où R(i,j) == 1\n",
    "    diff = (predictions - Y) * R\n",
    "    \n",
    "    # Terme d'erreur quadratique\n",
    "    cost = 0.5 * torch.sum(diff**2)\n",
    "    \n",
    "    # Termes de régularisation L2\n",
    "    reg = (lambda_ / 2) * (torch.sum(X**2) + torch.sum(W**2))\n",
    "    \n",
    "    return cost + reg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2ad93c",
   "metadata": {},
   "source": [
    "### 3.4). Processus d'optimisation par descente de gradient\n",
    "\n",
    "Nous exécutons manuellement la boucle d'entraînement. À chaque itération :\n",
    "\n",
    "- Nous calculons la perte (*loss*) via la fonction de coût.\n",
    "\n",
    "- Nous appelons `loss.backward()` pour générer les gradients par rapport à $X$, $W$ et $b$.\n",
    "\n",
    "- Nous mettons à jour les paramètres via `optimizer.step()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad821015",
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = 500\n",
    "lambda_reg = 0.01\n",
    "\n",
    "print(\"Début de l'entraînement manuel...\")\n",
    "for i in range(iterations):\n",
    "    # Remise à zéro des gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Calcul du coût\n",
    "    loss = cofi_cost_func(X, W, b, Y, R, lambda_reg)\n",
    "    \n",
    "    # Calcul automatique des gradients\n",
    "    loss.backward()\n",
    "    \n",
    "    # Mise à jour des paramètres X, W et b\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    \n",
    "    if i % 20 == 0:\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        print(f\"Itération {i} : Coût = {loss.item():.2f}, Learning rate : {current_lr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59cc3982",
   "metadata": {},
   "source": [
    "### 3.5). Génération des recommandations\n",
    "\n",
    "Une fois les matrices $X$, $W$ et le biais $b$ optimisés, nous pouvons prédire la note pour n'importe quel film $i$ et utilisateur $j$ en calculant simplement :\n",
    "\n",
    "$$\n",
    "\\hat{Y} = X W^{\\mathsf{T}} + b\n",
    "$$\n",
    "\n",
    "Ces prédictions nous permettent de recommander les films ayant les notes prédites les plus élevées que l'utilisateur n'a pas encore vus.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095e4a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def voir_comparaison_utilisateur(user_id, X, W, b, Y_matrix, movieList, k=10):\n",
    "    # On récupère les paramètres optimisés pour cet utilisateur\n",
    "    with torch.no_grad():\n",
    "        # Prédiction pour l'utilisateur spécifique : X @ w_j.t() + b_j\n",
    "        # W[user_id] est de taille (nf,), on le reshape pour le produit matriciel\n",
    "        w_user = W[user_id].reshape(1, -1)\n",
    "        b_user = b[0, user_id]\n",
    "        user_preds = (X @ w_user.t()) + b_user\n",
    "        user_preds = user_preds.numpy().flatten()\n",
    "\n",
    "    # Trouver les indices des films que l'utilisateur a réellement notés\n",
    "    rated_indices = np.where(Y_matrix[:, user_id] > 0)[0]\n",
    "    \n",
    "    print(f\"--- Comparaison des notes pour l'utilisateur ID: {user_id} ---\")\n",
    "    print(f\"{'Film':<50} | {'Réelle':<7} | {'Prédite':<7}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    for idx in rated_indices[:k]: # On affiche les k premiers\n",
    "        print(f\"{movieList[idx][:50]:<50} | {Y_matrix[idx, user_id]:<7.1f} | {user_preds[idx]:<7.2f}\")\n",
    "\n",
    "# Utilisation\n",
    "movie_titles_df = df_users[['MovieID', 'Title']].drop_duplicates().set_index('MovieID')\n",
    "movieList = [movie_titles_df.loc[mid, 'Title'] for mid in Y_df.index]\n",
    "voir_comparaison_utilisateur(2, X, W, b, Y_matrix, movieList)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f27e35",
   "metadata": {},
   "source": [
    "### 3.7). Recommandation pour un nouvel utilisateur\n",
    "\n",
    "Lorsqu'un nouvel utilisateur arrive, nous ne connaissons pas son vecteur de préférences $w_{\\text{new}}$. Au lieu de réentraîner tout le modèle, nous utilisons une approche d'optimisation locale :\n",
    "\n",
    "- Nous fixons la matrice des caractéristiques des films $X$ (déjà apprise).\n",
    "\n",
    "- Nous initialisons un nouveau vecteur $w_{\\text{new}}$ et un biais $b_{\\text{new}}$.\n",
    "\n",
    "- Nous optimisons uniquement ces deux paramètres pour minimiser l'erreur sur les quelques films notés par ce nouvel utilisateur :\n",
    "\n",
    "$$\n",
    "J_{\\text{new}}(w_{\\text{new}}, b_{\\text{new}})\n",
    "=\n",
    "\\frac{1}{2}\n",
    "\\sum_{i:\\,R(i)=1}\n",
    "\\left( x^{(i)} \\cdot w_{\\text{new}} + b_{\\text{new}} - y(i) \\right)^2\n",
    "+\n",
    "\\frac{\\lambda}{2}\n",
    "\\lVert w_{\\text{new}} \\rVert^2\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9251ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_recommendations_cofi(movie_index, X_trained, movieList, k=5):\n",
    "    \"\"\"\n",
    "    Trouve les k films les plus similaires à un film donné en utilisant \n",
    "    la similarité cosinus sur les vecteurs latents X.\n",
    "\n",
    "    cofi : collaborative filtering\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        # Extraction du vecteur du film cible\n",
    "        v_cible = X_trained[movie_index].reshape(1, -1)\n",
    "        \n",
    "        # Calcul de la similarité cosinus entre le film cible et tous les autres\n",
    "        similarites = F.cosine_similarity(v_cible, X_trained)\n",
    "        \n",
    "    # On récupère les indices des films les plus similaires (tri décroissant)\n",
    "    indices_proches = torch.argsort(similarites, descending=True)\n",
    "    \n",
    "    print(f\"\\n--- Films les plus proches de : {movieList[movie_index]} ---\")\n",
    "    count = 0\n",
    "    for idx in indices_proches:\n",
    "        # On ignore le film lui-même\n",
    "        if idx.item() == movie_index:\n",
    "            continue\n",
    "            \n",
    "        print(f\"Score de similarité: {similarites[idx].item():.4f} | Film: {movieList[idx]}\")\n",
    "        count += 1\n",
    "        if count == k:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aeefdd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Exemple d'utilisation ---\n",
    "\n",
    "# 1. On cherche d'abord les infos sur le film\n",
    "# On suppose que movieIDs est une liste des MovieID correspondant à movieList\n",
    "movieIDs = Y_df.index.tolist()\n",
    "recherches = search_films(\"Toy Story\", movieList, movieIDs)\n",
    "\n",
    "# 2. Si on a trouvé le film, on cherche les meilleurs ajustements pour le premier résultat\n",
    "if recherches:\n",
    "    target_idx = recherches[0]['index']\n",
    "    k_recommendations_cofi(target_idx, X.detach(), movieList, k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504231e7",
   "metadata": {},
   "source": [
    "## 4). Modelling : content-based filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8616e3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad2caf2",
   "metadata": {},
   "source": [
    "### 4.1). Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f68526",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Prepare genres: Ensure they are lists and explode\n",
    "df_exploded = df_users.copy()\n",
    "df_exploded['Genres'] = df_exploded['Genres'].apply(lambda x: x if isinstance(x, list) else [])\n",
    "df_exploded = df_exploded.explode('Genres')\n",
    "\n",
    "# Remove any empty genre strings if they exist\n",
    "df_exploded = df_exploded[df_exploded['Genres'] != \"\"]\n",
    "\n",
    "# 2. Calculate average rating per user per genre\n",
    "user_genre_ave = df_exploded.pivot_table(\n",
    "    index='UserID', \n",
    "    columns='Genres', \n",
    "    values='Rating', \n",
    "    aggfunc='mean'\n",
    ").fillna(0.0)\n",
    "\n",
    "# 3. Calculate total ratings and global average per user\n",
    "user_stats = df_users.groupby('UserID')['Rating'].agg(['count', 'mean']).rename(\n",
    "    columns={'mean': 'rating ave', 'count': 'rating count'}\n",
    ")\n",
    "\n",
    "# 4. Final User Table\n",
    "user_table = user_stats.join(user_genre_ave)\n",
    "\n",
    "print(\"--- User Table ---\")\n",
    "display(user_table.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d66cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Calculate average rating per movie\n",
    "movie_stats = df_users.groupby('MovieID')['Rating'].mean().rename('ave rating')\n",
    "\n",
    "# 3. Create genre flags with 0 and 1\n",
    "genre_flags = pd.get_dummies(\n",
    "    df_exploded.set_index('MovieID')['Genres'], \n",
    "    dtype=int\n",
    ").groupby('MovieID').max()\n",
    "\n",
    "# 4. Combine with title and year\n",
    "movie_info = df_users[['MovieID', 'Title', 'Year']].drop_duplicates().set_index('MovieID')\n",
    "movie_table = movie_info.join(movie_stats).join(genre_flags)\n",
    "\n",
    "print(\"\\n--- Movie Tabl ---\")\n",
    "display(movie_table.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24bc2e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need a row for every rating in the original dataset\n",
    "# Each row contains: User features, Movie features, Target Rating\n",
    "merged_data = df_users[['UserID', 'MovieID', 'Rating']].copy()\n",
    "\n",
    "# Join user features\n",
    "user_vecs = merged_data[['UserID']].merge(user_table, left_on='UserID', right_index=True).drop(columns=['UserID', 'rating count', 'rating ave']).values\n",
    "# Join movie features\n",
    "movie_vecs = merged_data[['MovieID']].merge(movie_table, left_on='MovieID', right_index=True).drop(columns=['MovieID', 'Title']).values\n",
    "# Targets\n",
    "y_vecs = merged_data['Rating'].values.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_vecs.shape)\n",
    "print(user_vecs.shape)\n",
    "print(movie_vecs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a2cfcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling the data \n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# Chemins pour les scalers\n",
    "scaler_user_path = 'scaler_user.joblib'\n",
    "scaler_item_path = 'scaler_item.joblib'\n",
    "scaler_target_path = 'scaler_target.joblib'\n",
    "\n",
    "# Si les fichiers existent, on les charge, sinon on les entraîne\n",
    "if os.path.exists(scaler_user_path):\n",
    "    scalerUser = joblib.load(scaler_user_path)\n",
    "    scalerItem = joblib.load(scaler_item_path)\n",
    "    scalerTarget = joblib.load(scaler_target_path)\n",
    "    user_train_scaled = scalerUser.transform(user_vecs) # On utilise transform au lieu de fit_transform\n",
    "    item_train_scaled = scalerItem.transform(movie_vecs)\n",
    "    y_train_scaled = scalerTarget.transform(y_vecs.reshape(-1, 1))\n",
    "    print(\"Scalers chargés depuis le disque.\")\n",
    "else:\n",
    "    scalerUser = StandardScaler()\n",
    "    user_train_scaled = scalerUser.fit_transform(user_vecs)\n",
    "\n",
    "    scalerItem = StandardScaler()\n",
    "    item_train_scaled = scalerItem.fit_transform(movie_vecs)\n",
    "\n",
    "    scalerTarget = StandardScaler()\n",
    "    y_train_scaled = scalerTarget.fit_transform(y_vecs.reshape(-1, 1))\n",
    "    \n",
    "    # Sauvegarde\n",
    "    joblib.dump(scalerUser, scaler_user_path)\n",
    "    joblib.dump(scalerItem, scaler_item_path)\n",
    "    joblib.dump(scalerTarget, scaler_target_path)\n",
    "    print(\"Scalers entraînés et sauvegardés.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893313f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. SPLIT THE DATA (80/20) ---\n",
    "user_train, user_test, item_train, item_test, y_train, y_test = train_test_split(\n",
    "    user_train_scaled, item_train_scaled, y_train_scaled, \n",
    "    test_size=0.20, shuffle=True, random_state=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38350d2",
   "metadata": {},
   "source": [
    "### 4.2). Creating the dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4ce9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecSysDataset(Dataset):\n",
    "    def __init__(self, user_data, item_data, target_data):\n",
    "        self.user_data = torch.tensor(user_data, dtype=torch.float32)\n",
    "        self.item_data = torch.tensor(item_data, dtype=torch.float32)\n",
    "        self.targets = torch.tensor(target_data, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.user_data[idx], self.item_data[idx], self.targets[idx]\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataset = RecSysDataset(user_train, item_train, y_train)\n",
    "test_dataset = RecSysDataset(user_test, item_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False)\n",
    "\n",
    "# Identify feature counts for your model\n",
    "num_user_features = user_train.shape[1]\n",
    "num_item_features = item_train.shape[1]\n",
    "\n",
    "print(f\"Num User Features: {num_user_features}\")\n",
    "print(f\"Num Item Features: {num_item_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb6f61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the sizes \n",
    "user_batch, item_batch, y_batch = next(iter(train_loader))\n",
    "print(user_batch.shape)\n",
    "print(item_batch.shape)\n",
    "print(y_batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1f0cbb",
   "metadata": {},
   "source": [
    "### 4.2). Creating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd8f509",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createTheRecommenderNet(num_user_features, num_item_features, num_outputs=32): \n",
    "\n",
    "    class TwoTowerModel(nn.Module):\n",
    "        def __init__(self, num_user_features, num_item_features, num_outputs):\n",
    "            super(TwoTowerModel, self).__init__()\n",
    "            \n",
    "            # User Network\n",
    "            self.user_nn = nn.Sequential(\n",
    "                nn.Linear(num_user_features, 128),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(128, 64),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(64, num_outputs)\n",
    "            )\n",
    "            \n",
    "            # Item Network\n",
    "            self.item_nn = nn.Sequential(\n",
    "                nn.Linear(num_item_features, 128),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(128, 64),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(64, num_outputs)\n",
    "            )\n",
    "\n",
    "        def forward(self, user_input, item_input):\n",
    "            # Pass through networks\n",
    "            vu = self.user_nn(user_input)\n",
    "            vm = self.item_nn(item_input)\n",
    "            \n",
    "            # L2 Normalization: p=2 is the L2 norm, dim=1 is the feature axis\n",
    "            vu = F.normalize(vu, p=2, dim=1)\n",
    "            vm = F.normalize(vm, p=2, dim=1)\n",
    "            \n",
    "            # Dot product (batch-wise)\n",
    "            # Using (vu * vm).sum(dim=1) or torch.bmm\n",
    "            output = torch.sum(vu * vm, dim=1, keepdim=True)\n",
    "            \n",
    "            return output\n",
    "\n",
    "    # create the model instance\n",
    "    net = TwoTowerModel(num_user_features, num_item_features, num_outputs)\n",
    "    \n",
    "    # loss function\n",
    "    lossfun = nn.MSELoss()\n",
    "\n",
    "    # optimizer\n",
    "    optimizer = torch.optim.Adam(net.parameters(),lr=.001)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.5)\n",
    "\n",
    "    return net,lossfun,optimizer,scheduler \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1496a207",
   "metadata": {},
   "outputs": [],
   "source": [
    "createTheRecommenderNet(num_user_features,num_item_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f9c7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the model with one batch\n",
    "net,lossfun,optimizer, scheduler = createTheRecommenderNet(num_user_features, num_item_features)\n",
    "\n",
    "user_batch, movie_batch, y_batch = next(iter(train_loader))\n",
    "yHat = net(user_batch, movie_batch)\n",
    "\n",
    "loss = lossfun(yHat,y_batch)\n",
    "print(' ')\n",
    "print('Loss:')\n",
    "print(loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d841bb",
   "metadata": {},
   "source": [
    "### 4.3). Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76df7ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def function2trainTheModel():\n",
    "\n",
    "  # number of epochs\n",
    "  numepochs = 10\n",
    "  \n",
    "  # create a new model\n",
    "  net,lossfun,optimizer, scheduler = createTheRecommenderNet(num_user_features, num_item_features)\n",
    "\n",
    "  # initialize losses\n",
    "  losses    = torch.zeros(numepochs)\n",
    "\n",
    "  # loop over epochs\n",
    "  for epochi in range(numepochs):\n",
    "\n",
    "    # loop over training data batches\n",
    "    batchLoss = []\n",
    "    for user_vec, movie_vec, y in train_loader:\n",
    "\n",
    "      # forward pass and loss\n",
    "      yHat = net(user_vec, movie_vec)\n",
    "      loss = lossfun(yHat,y)\n",
    "\n",
    "      # backprop\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      # loss from this batch\n",
    "      batchLoss.append(loss.item())\n",
    "\n",
    "    # end of batch loop...\n",
    "\n",
    "    # and get average losses across the batches\n",
    "    scheduler.step()\n",
    "    epoch_loss = np.mean(batchLoss)\n",
    "    losses[epochi] = epoch_loss\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    print(f'Epoch {epochi} is completed, the loss is {epoch_loss}, the learning rate is {current_lr}')\n",
    "      \n",
    "  # end epochs\n",
    "\n",
    "  # function output\n",
    "  return losses,net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2712c372",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'content_based_model.pth'\n",
    "force_train = False  # Changez à True si vous voulez absolument ré-entraîner le modèle\n",
    "\n",
    "# Initialisation de l'architecture (indispensable pour charger les poids)\n",
    "net, lossfun, optimizer, scheduler = createTheRecommenderNet(num_user_features, num_item_features)\n",
    "\n",
    "if os.path.exists(model_path) and not force_train:\n",
    "    print(\"Chargement des poids du modèle sauvegardé...\")\n",
    "    # Charger les poids dans l'instance 'net'\n",
    "    net.load_state_dict(torch.load(model_path))\n",
    "    net.eval() # Mode évaluation\n",
    "    print(\"Modèle prêt à l'emploi (entraînement sauté) !\")\n",
    "else:\n",
    "    print(\"Aucun modèle trouvé ou force_train=True. Lancement de l'entraînement (30 min)...\")\n",
    "    losses, net = function2trainTheModel()\n",
    "    # Sauvegarde des poids après l'entraînement\n",
    "    torch.save(net.state_dict(), model_path)\n",
    "    print(f\"Modèle entraîné et sauvegardé sous {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411508a8",
   "metadata": {},
   "source": [
    "###  4.4). Generating predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f10856",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_content_based(user_ratings_dict, net, movie_table, scalerUser, scalerItem, scalerTarget, k=10):\n",
    "    \"\"\"\n",
    "    user_ratings_dict: e.g., {\"Casablanca (1942)\": 5.0, \"Toy Story (1995)\": 1.0}\n",
    "    \"\"\"\n",
    "    net.eval()\n",
    "    \n",
    "    # 1. Map ratings to MovieIDs to find genres\n",
    "    # Note: movie_table index is MovieID\n",
    "    genre_cols = movie_table.columns[3:] # Action, Adventure, etc.\n",
    "    user_genre_ratings = {g: [] for g in genre_cols}\n",
    "    rated_movie_ids = []\n",
    "\n",
    "    for title, rating in user_ratings_dict.items():\n",
    "        # Find MovieID for title\n",
    "        match = movie_table[movie_table['Title'] == title]\n",
    "        if match.empty:\n",
    "            print(f\"Movie '{title}' not found in database.\")\n",
    "            continue\n",
    "        \n",
    "        m_id = match.index[0]\n",
    "        rated_movie_ids.append(m_id)\n",
    "        \n",
    "        # Check which genres this movie has and append the rating\n",
    "        for g in genre_cols:\n",
    "            if match.iloc[0][g] == 1:\n",
    "                user_genre_ratings[g].append(rating)\n",
    "\n",
    "    # 2. Construct the user feature vector (18 features: 0 or average per genre)\n",
    "    # Based on your user_table logic: [Action, Adventure, ..., Western]\n",
    "    user_features = []\n",
    "    for g in genre_cols:\n",
    "        if len(user_genre_ratings[g]) > 0:\n",
    "            user_features.append(np.mean(user_genre_ratings[g]))\n",
    "        else:\n",
    "            user_features.append(0.0)\n",
    "    \n",
    "    user_vec = np.array(user_features).reshape(1, -1)\n",
    "    \n",
    "    # 3. Scale User Vector\n",
    "    user_vec_scaled = scalerUser.transform(user_vec)\n",
    "    user_tensor = torch.tensor(user_vec_scaled, dtype=torch.float32)\n",
    "\n",
    "    # 4. Prepare Movie Features (All Movies)\n",
    "    movie_features = movie_table.drop(columns=['Title', 'Year', 'ave rating']).values\n",
    "    movie_features_scaled = scalerItem.transform(movie_table.drop(columns=['Title']).values)\n",
    "    movie_tensor = torch.tensor(movie_features_scaled, dtype=torch.float32)\n",
    "\n",
    "    # 5. Predict\n",
    "    with torch.no_grad():\n",
    "        # Repeat user tensor to match number of movies\n",
    "        u_expanded = user_tensor.repeat(movie_tensor.size(0), 1)\n",
    "        preds_scaled = net(u_expanded, movie_tensor)\n",
    "        preds = scalerTarget.inverse_transform(preds_scaled.numpy())\n",
    "\n",
    "    # 6. Sort and Filter\n",
    "    movie_table_copy = movie_table.copy()\n",
    "    movie_table_copy['prediction'] = preds\n",
    "    \n",
    "    # Exclude movies already rated\n",
    "    recommendations = movie_table_copy[~movie_table_copy.index.isin(rated_movie_ids)]\n",
    "    top_k = recommendations.sort_values(by='prediction', ascending=False).head(k)\n",
    "\n",
    "    print(f\"\\n--- Top {k} Content-Based Recommendations ---\")\n",
    "    for i, row in top_k.iterrows():\n",
    "        print(f\"Predicted: {row['prediction']:.2f} | {row['Title']}\")\n",
    "        \n",
    "    return top_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886bda38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of usage \n",
    "user_ratings_dict = {\"Forrest Gump\": 5.0}\n",
    "recommend_content_based(user_ratings_dict, net, movie_table, scalerUser, scalerItem, scalerTarget)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
